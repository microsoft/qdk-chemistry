#!/usr/bin/env python3
"""
Profile memory usage per C++ file during compilation.

This script uses compile_commands.json (generated by CMake) to get the exact
compilation command for each file, then measures peak memory usage.

Usage:
    # First, generate compile_commands.json:
    cd build && cmake -DCMAKE_EXPORT_COMPILE_COMMANDS=ON ..

    # Then run profiling:
    python profile_compile_memory.py [--build-dir BUILD_DIR] [--top N]

    # Parallel execution (faster but uses more memory):
    python profile_compile_memory.py --jobs 4
"""
# --------------------------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE.txt in the project root for license information.
# --------------------------------------------------------------------------------------------

import argparse
import json
import os
import subprocess
import sys
import time
from concurrent.futures import ProcessPoolExecutor, as_completed
from multiprocessing import cpu_count
from pathlib import Path
from dataclasses import dataclass


@dataclass
class CompileResult:
    source_file: str
    peak_memory_kb: int
    compile_time_sec: float
    success: bool
    error_msg: str = ""


def compile_with_memory_tracking(entry: dict, timeout: int = 600) -> CompileResult:
    """Run compilation command and track memory usage using os.wait4/getrusage."""

    source_file = entry.get("file", "unknown")
    directory = entry.get("directory", ".")

    # Get command - can be 'command' (string) or 'arguments' (list)
    if "arguments" in entry:
        command = entry["arguments"]
    elif "command" in entry:
        import shlex

        command = shlex.split(entry["command"])
    else:
        return CompileResult(
            source_file=source_file,
            peak_memory_kb=0,
            compile_time_sec=0,
            success=False,
            error_msg="No command in entry",
        )

    start_time = time.time()

    try:
        # Modify command to output to /dev/null to avoid disk I/O affecting results
        modified_cmd = []
        skip_next = False
        for i, arg in enumerate(command):
            if skip_next:
                skip_next = False
                continue
            if arg == "-o" and i + 1 < len(command):
                modified_cmd.extend(["-o", "/dev/null"])
                skip_next = True
            else:
                modified_cmd.append(arg)

        proc = subprocess.Popen(
            modified_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=directory
        )

        # Use wait4 with WNOHANG in a loop for timeout support
        deadline = time.time() + timeout
        peak_memory_kb = 0

        while True:
            try:
                pid, status, rusage = os.wait4(proc.pid, os.WNOHANG)
                if pid != 0:
                    # Process finished
                    peak_memory_kb = rusage.ru_maxrss
                    elapsed = time.time() - start_time
                    exit_code = os.WEXITSTATUS(status) if os.WIFEXITED(status) else -1
                    break
            except ChildProcessError:
                # Process already reaped
                elapsed = time.time() - start_time
                exit_code = -1
                break

            # Check if process is still alive (might have been OOM killed)
            if proc.poll() is not None:
                elapsed = time.time() - start_time
                exit_code = proc.returncode
                break

            if time.time() > deadline:
                # Timeout - kill the process
                try:
                    proc.kill()
                    os.wait4(proc.pid, 0)  # Reap zombie
                except Exception:
                    # If kill/reap fails, we'll fall through and report timeout
                    pass
                return CompileResult(
                    source_file=source_file,
                    peak_memory_kb=0,
                    compile_time_sec=timeout,
                    success=False,
                    error_msg=f"TIMEOUT after {timeout}s",
                )

            time.sleep(0.5)  # Longer sleep to reduce CPU usage

        # Read any remaining output
        _, stderr = proc.stdout.read(), proc.stderr.read()

        if exit_code != 0:
            return CompileResult(
                source_file=source_file,
                peak_memory_kb=peak_memory_kb,
                compile_time_sec=elapsed,
                success=False,
                error_msg=stderr.decode()[:500],
            )

        return CompileResult(
            source_file=source_file,
            peak_memory_kb=peak_memory_kb,
            compile_time_sec=elapsed,
            success=True,
        )

    except Exception as e:
        return CompileResult(
            source_file=source_file,
            peak_memory_kb=0,
            compile_time_sec=0,
            success=False,
            error_msg=str(e),
        )


def parse_compile_commands(json_path: Path) -> list[dict]:
    """Parse compile_commands.json and return list of compilation entries."""
    with open(json_path, "r") as f:
        return json.load(f)


def main():
    parser = argparse.ArgumentParser(
        description="Profile memory usage per C++ file during compilation"
    )
    parser.add_argument(
        "--build-dir",
        "-b",
        default="build",
        help="Path to the CMake build directory (default: build)",
    )
    parser.add_argument(
        "--filter",
        "-f",
        default=None,
        help='Only profile files matching this pattern (e.g., "src/qdk" or "macis")',
    )
    parser.add_argument(
        "--exclude",
        "-e",
        nargs="+",
        default=["_deps"],
        help="Exclude files matching these patterns (default: _deps)",
    )
    parser.add_argument(
        "--include-deps",
        action="store_true",
        help="Include _deps directory (FetchContent dependencies)",
    )
    parser.add_argument(
        "--jobs",
        "-j",
        type=int,
        default=1,
        help="Number of parallel compilation jobs (default: 1, sequential)",
    )
    parser.add_argument(
        "--top",
        "-n",
        type=int,
        default=20,
        help="Show top N memory consumers (default: 20)",
    )
    parser.add_argument(
        "--output",
        "-o",
        default="memory_profile_results.txt",
        help="Output file for results (default: memory_profile_results.txt)",
    )
    parser.add_argument(
        "--threshold",
        "-t",
        type=int,
        default=0,
        help="Only show files using more than N MB (default: 0, show all)",
    )
    parser.add_argument(
        "--timeout",
        "-T",
        type=int,
        default=600,
        help="Timeout per file in seconds (default: 600 = 10 minutes)",
    )

    args = parser.parse_args()

    # Resolve paths
    script_dir = Path(__file__).parent.resolve()
    build_dir = Path(args.build_dir)
    if not build_dir.is_absolute():
        build_dir = script_dir / build_dir

    # Find compile_commands.json
    compile_commands_path = build_dir / "compile_commands.json"
    if not compile_commands_path.exists():
        print(f"Error: {compile_commands_path} not found!")
        print()
        print("Generate it by running cmake with:")
        print(f"  cd {build_dir} && cmake -DCMAKE_EXPORT_COMPILE_COMMANDS=ON ..")
        print()
        print("Or add to your CMakeLists.txt:")
        print("  set(CMAKE_EXPORT_COMPILE_COMMANDS ON)")
        sys.exit(1)

    # Parse compile commands
    print(f"Reading {compile_commands_path}")
    entries = parse_compile_commands(compile_commands_path)

    # Filter entries
    if args.filter:
        entries = [e for e in entries if args.filter in e.get("file", "")]

    # Handle --include-deps flag
    excludes = (
        args.exclude
        if not args.include_deps
        else [e for e in args.exclude if e != "_deps"]
    )

    for excl in excludes:
        entries = [e for e in entries if excl not in e.get("file", "")]

    num_jobs = args.jobs if args.jobs > 0 else cpu_count()
    print(f"Found {len(entries)} compilation units to profile")
    print(f"Using {num_jobs} parallel job(s)")
    print("=" * 70)

    results: list[CompileResult] = []
    failed_errors: list[tuple[str, str]] = []

    if num_jobs == 1:
        # Sequential execution with progress
        for i, entry in enumerate(entries, 1):
            source_file = entry.get("file", "unknown")

            try:
                rel_path = Path(source_file).relative_to(script_dir.parent)
            except ValueError:
                rel_path = Path(source_file)

            print(f"[{i}/{len(entries)}] {rel_path} ... ", end="", flush=True)

            result = compile_with_memory_tracking(entry, timeout=args.timeout)
            results.append(result)

            if result.success:
                mb = result.peak_memory_kb / 1024
                print(f"{mb:.0f} MB ({result.compile_time_sec:.1f}s)")
            else:
                print(f"FAILED: {result.error_msg[:60]}")
                first_error = (
                    result.error_msg.split("\n")[0][:80]
                    if result.error_msg
                    else "unknown"
                )
                failed_errors.append((str(rel_path), first_error))
    else:
        # Parallel execution
        from functools import partial

        compile_fn = partial(compile_with_memory_tracking, timeout=args.timeout)

        completed = 0
        with ProcessPoolExecutor(max_workers=num_jobs) as executor:
            future_to_entry = {executor.submit(compile_fn, e): e for e in entries}

            for future in as_completed(future_to_entry):
                entry = future_to_entry[future]
                completed += 1
                source_file = entry.get("file", "unknown")

                try:
                    rel_path = Path(source_file).relative_to(script_dir.parent)
                except ValueError:
                    rel_path = Path(source_file)

                try:
                    result = future.result()
                    results.append(result)

                    if result.success:
                        mb = result.peak_memory_kb / 1024
                        print(
                            f"[{completed}/{len(entries)}] {rel_path} ... {mb:.0f} MB ({result.compile_time_sec:.1f}s)"
                        )
                    else:
                        print(
                            f"[{completed}/{len(entries)}] {rel_path} ... FAILED: {result.error_msg[:40]}"
                        )
                        first_error = (
                            result.error_msg.split("\n")[0][:80]
                            if result.error_msg
                            else "unknown"
                        )
                        failed_errors.append((str(rel_path), first_error))
                except Exception as e:
                    print(f"[{completed}/{len(entries)}] {rel_path} ... ERROR: {e}")

    print()
    print("=" * 70)
    print(f"TOP {args.top} MEMORY CONSUMERS")
    print("=" * 70)

    # Sort by peak memory, descending
    successful = [r for r in results if r.success and r.peak_memory_kb > 0]
    if args.threshold > 0:
        threshold_kb = args.threshold * 1024
        successful = [r for r in successful if r.peak_memory_kb >= threshold_kb]
    successful.sort(key=lambda x: x.peak_memory_kb, reverse=True)

    # Write results to file
    output_path = build_dir / args.output
    with open(output_path, "w") as f:
        f.write("# Compile Memory Profile Results\n")
        f.write("# Memory (MB) | Time (s) | File\n")
        f.write("#" + "=" * 70 + "\n")

        for r in successful:
            mb = r.peak_memory_kb / 1024
            f.write(f"{mb:8.0f} MB | {r.compile_time_sec:6.1f}s | {r.source_file}\n")

    # Print top N
    for i, r in enumerate(successful[: args.top], 1):
        mb = r.peak_memory_kb / 1024
        try:
            rel_path = Path(r.source_file).relative_to(script_dir.parent)
        except ValueError:
            rel_path = Path(r.source_file)
        print(f"{i:3}. {mb:7.0f} MB  ({r.compile_time_sec:5.1f}s)  {rel_path}")

    print()
    print(f"Full results saved to: {output_path}")

    # Summary stats
    if successful:
        total_mb = sum(r.peak_memory_kb for r in successful) / 1024
        max_mb = max(r.peak_memory_kb for r in successful) / 1024
        avg_mb = total_mb / len(successful)

        print()
        print("Summary:")
        print(f"  Files compiled: {len(successful)}/{len(results)}")
        print(f"  Max memory:     {max_mb:.0f} MB")
        print(f"  Avg memory:     {avg_mb:.0f} MB")

        # Warn about files > 2GB
        heavy = [r for r in successful if r.peak_memory_kb > 2 * 1024 * 1024]
        if heavy:
            print()
            print(f"⚠️  WARNING: {len(heavy)} file(s) use more than 2GB RAM:")
            for r in heavy:
                mb = r.peak_memory_kb / 1024
                try:
                    rel_path = Path(r.source_file).relative_to(script_dir.parent)
                except ValueError:
                    rel_path = r.source_file
                print(f"    {mb:.0f} MB - {rel_path}")

    # Show failed files summary
    if failed_errors:
        print()
        print(f"⚠️  {len(failed_errors)} file(s) failed to compile:")
        for path, err in failed_errors[:10]:
            print(f"    {path}")
            print(f"      -> {err}")
        if len(failed_errors) > 10:
            print(f"    ... and {len(failed_errors) - 10} more")


if __name__ == "__main__":
    main()
